## 卷积过程
每一个卷积核的**通道数量**，必须要求与输入**通道数量**一致，因为每个卷积核的对应**通道**与输入的对应**通道**进行卷积运算
![卷积过程](https://github.com/SZUZOUXu/Deep-Learning/blob/main/image/%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B.png)

如果要卷积后也输出多通道，增加卷积核（filers）的数量即可，相当于不同权重的卷积核去计算，示意图如下：
![输出多通道卷积](https://github.com/SZUZOUXu/Deep-Learning/blob/main/image/%E8%BE%93%E5%87%BA%E5%A4%9A%E9%80%9A%E9%81%93%E5%8D%B7%E7%A7%AF.png)

1×1卷积先降维再升维的特点：

第一个1x1的卷积把256维channel降到64维（64个卷积核），然后在最后通过1x1卷积恢复，参数数目：1x1x256x64（每个卷积核1x1x256，共计64个） + 3x3x64x64 + 1x1x64x256 = 69632，

全连接层(左图)就是两个3x3x256的卷积，参数数目: 3x3x256x256x2 = 1179648，差了16.94倍。
![1×1卷积](https://github.com/SZUZOUXu/Deep-Learning/blob/main/image/1%C3%971%E5%8D%B7%E7%A7%AF.png)
经典卷积网络可以提取出优秀的特征，学习它们可以帮助我们建立出更优秀的网络结构
### VGG16
它的结构如下图所示：
![VGG模型](https://github.com/SZUZOUXu/Deep-Learning/blob/main/image/VGG%E6%A8%A1%E5%9E%8B.png)
convolution+Relu:卷积层，通常使用3×3窗口和步幅1

**卷积原理**
1. 输入特征图input_shape:(28,28,1)
2. 3×3窗口切分(3,3,1)（共有26个窗口）
3. 窗口与卷积核(w)点积->(output_depth,)的1D向量(1,1,32)
4. 组合起来输出特征图
max pooling:最大池化，通常使用2×2窗口和步幅2，从输入特征图中提取窗口，输出每个通道的最大值；

**最大池化功能**
1. 减少特征图的参数个数
2. 窗口中包含的输入信息更多，假如一直卷积而不池化，
- fully nected+Relu:全连接
- softmax:不同输出类别的概率分布
1. 一张原始图片被resize到(224,224,3)。
2. conv1两次[3,3]卷积网络，输出的特征层为64，输出为(224,224,64)，再2X2最大池化，输出net为(112,112,64)。
3. conv2两次[3,3]卷积网络，输出的特征层为128，输出net为(112,112,128)，再2X2最大池化，输出net为(56,56,128)。
4. conv3三次[3,3]卷积网络，输出的特征层为256，输出net为(56,56,256)，再2X2最大池化，输出net为(28,28,256)。
5. conv3三次[3,3]卷积网络，输出的特征层为256，输出net为(28,28,512)，再2X2最大池化，输出net为(14,14,512)。
6. conv3三次[3,3]卷积网络，输出的特征层为256，输出net为(14,14,512)，再2X2最大池化，输出net为(7,7,512)。
7. 利用卷积的方式模拟全连接层，效果等同，输出net为(1,1,4096)。共进行两次。
8. 利用卷积的方式模拟全连接层，效果等同，输出net为(1,1,1000)。
最后输出的就是每个类的预测。

```Python
#-------------------------------------------------------------#
#   vgg16的网络部分
#-------------------------------------------------------------#
import tensorflow as tf
# slim是一个使构建，训练，评估神经网络变得简单的库，slim提供了很多计算机视觉方面的著名模型（VGG, AlexNet等）；
# 通过组合slim中变量(variables)、网络层(layer)、前缀名(scope)，模型可以被简洁的定义。
# 创建slim对象
slim = tf.contrib.slim

def vgg_16(inputs,
           num_classes=1000,
           is_training=True,
           dropout_keep_prob=0.5,
           spatial_squeeze=True,
           scope='vgg_16'):
    # 代码里是每个层是如何拿到自己对应的模型参数呢?这个的关键是变量空间。
    # vgg_arg_scope()函数返回了一个scope参数空间
    # 其实scope我们也传入的是’vgg_16’，tf.variable_scope()的参数，前两个是 name_or_scope , default_name。默认名称是当name_or_scope为空时，使用的名称。
    with tf.variable_scope(scope, 'vgg_16', [inputs]):
        # 建立vgg_16的网络，所有变量都在vgg_16内

        # conv1两次[3,3]卷积网络，输出的特征层为64，输出为(224,224,64)
        net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')
        # 2X2最大池化，输出net为(112,112,64)
        net = slim.max_pool2d(net, [2, 2], scope='pool1')

        # conv2两次[3,3]卷积网络，输出的特征层为128，输出net为(112,112,128)
        net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')
        # 2X2最大池化，输出net为(56,56,128)
        net = slim.max_pool2d(net, [2, 2], scope='pool2')

        # conv3三次[3,3]卷积网络，输出的特征层为256，输出net为(56,56,256)
        net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')
        # 2X2最大池化，输出net为(28,28,256)
        net = slim.max_pool2d(net, [2, 2], scope='pool3')

        # conv3三次[3,3]卷积网络，输出的特征层为256，输出net为(28,28,512)
        net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')
        # 2X2最大池化，输出net为(14,14,512)
        net = slim.max_pool2d(net, [2, 2], scope='pool4')

        # conv3三次[3,3]卷积网络，输出的特征层为256，输出net为(14,14,512)
        net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')
        # 2X2最大池化，输出net为(7,7,512)
        net = slim.max_pool2d(net, [2, 2], scope='pool5')

        # 利用卷积的方式模拟全连接层，效果等同，输出net为(1,1,4096)
        net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')
        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                            scope='dropout6')
        # 利用卷积的方式模拟全连接层，效果等同，输出net为(1,1,4096)
        net = slim.conv2d(net, 4096, [1, 1], scope='fc7')
        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                            scope='dropout7')
        # 利用卷积的方式模拟全连接层，效果等同，输出net为(1,1,1000)
        net = slim.conv2d(net, num_classes, [1, 1],
                        activation_fn=None,
                        normalizer_fn=None,
                        scope='fc8')
        
        # 由于用卷积的方式模拟全连接层，所以输出需要平铺
        if spatial_squeeze:
            net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
        return net

```

