### 为什么采用梯度下降
深度网络是由许多非线性层(带有激活函数)堆叠而成，每一层非线性层可以视为一个非线性函数 f(x) ，因此整个深度网络可以视为一个复合的非线性多元函数。

优化深度网络就是为了寻找到合适的权值，满足损失取得最小值点，比如简单的损失函数平方差(MSE)

在数学中寻找最小值问题，采用梯度下降的方法再合适不过了。

### 正向传播、反向传播
1. 前向传播：将训练集数据输入经过隐藏层，最后到达输出层并输出结果。【输入层—隐藏层–输出层】
2. 反向传播：由于输入结果与输出结果有误差，则计算**估计值与实际值之间的误差（loss）**，并将该误差从输出层向隐藏层反向传播，直至传播到输入层。【输出层–隐藏层–输入层】
3. 权重更新：在反向传播的过程中，根据**误差调整各种参数的值**；不断迭代上述过程，直至收敛。
以逻辑回归的神经元为例：**z = w1x1+w2x2 + b**，带入激活函数a = f(z)中，记损失函数为l，反向传播的最终目的是修正权值w，那么我们让l(loss)对w(参数)求偏导：

$$
\frac{\partial l}{\partial w} = \frac{\partial l}{\partial z}\frac{\partial z}{\partial w}
$$

- 右半部分很好求，属于**前向传播**得到的结果：

$$
\frac{\partial z}{\partial w_1} = x1，
\frac{\partial z}{\partial w_2} = x2
$$

- 左半部分属于**后向传播**的过程：

$$
\frac{\partial l}{\partial z} = \frac{\partial l}{\partial a}\frac{\partial a}{\partial z} = \frac{\partial l}{\partial a}f^’(\mathbf{z})
$$

因此需要**反向传播损失函数l对激活函数a=f(z)**进行求导

### 梯度消失问题
如果此部分小于1，那么随着层数的增加求出的梯度更新的信息会以指数形式衰减，如果某一次等于0，那么相乘就全为0这就是梯度消失。
### 梯度爆炸问题
在反向传播对激活函数进行求导的时候，如果此部分大于1，那么随着层数的增加，求出的梯度的更新将以指数形式增加，发生梯度爆炸。

### 网络退化问题
假设已经有了一个最优化的网络结构，是18层，假设设计了多于18层的网络模型，往往模型很难将这多余层恒等映射的参数学习正确，那么就一定会不比最优化的18层网络结构性能好，这就是随着网络深度增加，模型会产生退化现象。它**不是由过拟合产生的**，而是由**冗余的网络层**学习了**不是恒等映射的参数**造成的。
## 激活函数
一个神经网络由层节点组成，并学习将输入的样本映射到输出。对于给定的节点，将输入乘以节点中的权重，并将其相加。此值称为节点的summed activation。然后，经过求和的激活通过一个激活函数转换并定义特定的输出或节点的“activation”。

最简单的激活函数被称为**线性激活**，其中根本没有应用任何转换。 一个仅由线性激活函数组成的网络很容易训练，但**不能学习复杂的映射函数**。线性激活函数仍然用于预测一个数量的网络的输出层(例如回归问题)。

**非线性激活函数**是更好的，因为它们允许节点在数据中学习更复杂的结构 。两个广泛使用的非线性激活函数是sigmoid 函数和双曲正切激活函数。
### sigmoid 函数
Logistic函数神经网络，传统上是一个非常受欢迎的神经网络激活函数。函数的输入被转换成介于0.0和1.0之间的值。大于1.0的输入被转换为值1.0，同样，小于0.0的值被折断为0.0。所有可能的输入函数的形状都是从0到0.5到1.0的 s 形。

公式为：S(x) = 1 / (1 + e^-x)图像类似于S型
### tanh 函数
形状类似sigmoid 函数的非线性激活函数，输出值介于-1.0和1.0之间。

Sigmoid和 tanh 函数的一个普遍问题是它们**值域饱和**了 。这意味着，大值突然变为1.0，小值突然变为 -1或0。此外，函数只对其输入**中间点周围的变化非常敏感**。
###  ReLU（Rectified Linear Activation Function）函数
看起来和行为都像一个线性函数，但实际上是一个非线性函数，允许学习数据中的复杂关系 。该函数还必须提供更灵敏的激活和输入，避免饱和。（**线性的不值域饱和，非线性的复杂映射**）
```
if input > 0:
  return input
else:
  return 0
```
对于大于零的值，这个函数是线性的，这意味着当使用反向传播训练神经网络时，它具有很多线性激活函数的理想特性。然而，它是一个非线性函数，因为负值总是作为零输出。（**正值线性，负值非线性**）
## 卷积过程
每一个卷积核的**通道数量**，必须要求与输入**通道数量**一致，因为每个卷积核的对应**通道**与输入的对应**通道**进行卷积运算
![卷积过程](https://github.com/SZUZOUXu/Deep-Learning/blob/main/image/%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B.png)

如果要卷积后也输出多通道，增加卷积核（filers）的数量即可，相当于不同权重的卷积核去计算，示意图如下：
![输出多通道卷积](https://github.com/SZUZOUXu/Deep-Learning/blob/main/image/%E8%BE%93%E5%87%BA%E5%A4%9A%E9%80%9A%E9%81%93%E5%8D%B7%E7%A7%AF.png)

1×1卷积先降维再升维的特点：

第一个1x1的卷积把256维channel降到64维（64个卷积核），然后在最后通过1x1卷积恢复，参数数目：1x1x256x64（每个卷积核1x1x256，共计64个） + 3x3x64x64 + 1x1x64x256 = 69632，

全连接层(左图)就是两个3x3x256的卷积，参数数目: 3x3x256x256x2 = 1179648，差了16.94倍。
![1×1卷积](https://github.com/SZUZOUXu/Deep-Learning/blob/main/image/1%C3%971%E5%8D%B7%E7%A7%AF.png)
经典卷积网络可以提取出优秀的特征，学习它们可以帮助我们建立出更优秀的网络结构
### VGG16
它的结构如下图所示：
![VGG模型](https://github.com/SZUZOUXu/Deep-Learning/blob/main/image/VGG%E6%A8%A1%E5%9E%8B.png)
convolution+Relu:卷积层，通常使用3×3窗口和步幅1

**卷积原理**
1. 输入特征图input_shape:(28,28,1)
2. 3×3窗口切分(3,3,1)（共有26个窗口）
3. 窗口与卷积核(w)点积->(output_depth,)的1D向量(1,1,32)
4. 组合起来输出特征图
max pooling:最大池化，通常使用2×2窗口和步幅2，从输入特征图中提取窗口，输出每个通道的最大值；

**最大池化功能**
1. 减少特征图的参数个数
2. 窗口中包含的输入信息更多，假如一直卷积而不池化，
- fully nected+Relu:全连接
- softmax:不同输出类别的概率分布
1. 一张原始图片被resize到(224,224,3)。
2. conv1两次[3,3]卷积网络，输出的特征层为64，输出为(224,224,64)，再2X2最大池化，输出net为(112,112,64)。
3. conv2两次[3,3]卷积网络，输出的特征层为128，输出net为(112,112,128)，再2X2最大池化，输出net为(56,56,128)。
4. conv3三次[3,3]卷积网络，输出的特征层为256，输出net为(56,56,256)，再2X2最大池化，输出net为(28,28,256)。
5. conv3三次[3,3]卷积网络，输出的特征层为256，输出net为(28,28,512)，再2X2最大池化，输出net为(14,14,512)。
6. conv3三次[3,3]卷积网络，输出的特征层为256，输出net为(14,14,512)，再2X2最大池化，输出net为(7,7,512)。
7. 利用卷积的方式模拟全连接层，效果等同，输出net为(1,1,4096)。共进行两次。
8. 利用卷积的方式模拟全连接层，效果等同，输出net为(1,1,1000)。
最后输出的就是每个类的预测。

```Python
#-------------------------------------------------------------#
#   vgg16的网络部分
#-------------------------------------------------------------#
import tensorflow as tf
# slim是一个使构建，训练，评估神经网络变得简单的库，slim提供了很多计算机视觉方面的著名模型（VGG, AlexNet等）；
# 通过组合slim中变量(variables)、网络层(layer)、前缀名(scope)，模型可以被简洁的定义。
# 创建slim对象
slim = tf.contrib.slim

def vgg_16(inputs,
           num_classes=1000,
           is_training=True,
           dropout_keep_prob=0.5,
           spatial_squeeze=True,
           scope='vgg_16'):
    # 代码里是每个层是如何拿到自己对应的模型参数呢?这个的关键是变量空间。
    # vgg_arg_scope()函数返回了一个scope参数空间
    # 其实scope我们也传入的是’vgg_16’，tf.variable_scope()的参数，前两个是 name_or_scope , default_name。默认名称是当name_or_scope为空时，使用的名称。
    with tf.variable_scope(scope, 'vgg_16', [inputs]):
        # 建立vgg_16的网络，所有变量都在vgg_16内

        # conv1两次[3,3]卷积网络，输出的特征层为64，输出为(224,224,64)
        net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')
        # 2X2最大池化，输出net为(112,112,64)
        net = slim.max_pool2d(net, [2, 2], scope='pool1')

        # conv2两次[3,3]卷积网络，输出的特征层为128，输出net为(112,112,128)
        net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')
        # 2X2最大池化，输出net为(56,56,128)
        net = slim.max_pool2d(net, [2, 2], scope='pool2')

        # conv3三次[3,3]卷积网络，输出的特征层为256，输出net为(56,56,256)
        net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')
        # 2X2最大池化，输出net为(28,28,256)
        net = slim.max_pool2d(net, [2, 2], scope='pool3')

        # conv3三次[3,3]卷积网络，输出的特征层为256，输出net为(28,28,512)
        net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')
        # 2X2最大池化，输出net为(14,14,512)
        net = slim.max_pool2d(net, [2, 2], scope='pool4')

        # conv3三次[3,3]卷积网络，输出的特征层为256，输出net为(14,14,512)
        net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')
        # 2X2最大池化，输出net为(7,7,512)
        net = slim.max_pool2d(net, [2, 2], scope='pool5')

        # 利用卷积的方式模拟全连接层，效果等同，输出net为(1,1,4096)
        net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')
        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                            scope='dropout6')
        # 利用卷积的方式模拟全连接层，效果等同，输出net为(1,1,4096)
        net = slim.conv2d(net, 4096, [1, 1], scope='fc7')
        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                            scope='dropout7')
        # 利用卷积的方式模拟全连接层，效果等同，输出net为(1,1,1000)
        net = slim.conv2d(net, num_classes, [1, 1],
                        activation_fn=None,
                        normalizer_fn=None,
                        scope='fc8')
        
        # 由于用卷积的方式模拟全连接层，所以输出需要平铺
        if spatial_squeeze:
            net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
        return net

```

